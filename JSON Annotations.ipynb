{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting annotations from XML file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "\n",
    "xml_root_dir = \"LIDC-XML-only\"\n",
    "\n",
    "def extract_annotations(xml_file):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    namespace = {'ns': 'http://www.nih.gov'}\n",
    "    annotations_dict = {}\n",
    "    reading_sessions = root.findall('.//ns:readingSession', namespace)\n",
    "    for reading_session in reading_sessions:\n",
    "        for unblinded_read_nodule in reading_session.findall('.//ns:unblindedReadNodule', namespace):\n",
    "            nodule_id = unblinded_read_nodule.find('ns:noduleID', namespace).text\n",
    "            characteristics = unblinded_read_nodule.find('ns:characteristics', namespace)\n",
    "            characteristics_dict = {}\n",
    "            if characteristics is not None:\n",
    "                for characteristic in characteristics:\n",
    "                    characteristics_dict[characteristic.tag.split('}')[1]] = int(characteristic.text)\n",
    "            else:\n",
    "                continue\n",
    "            for roi in unblinded_read_nodule.findall('.//ns:roi', namespace):\n",
    "                image_sop_uid = roi.find('ns:imageSOP_UID', namespace).text\n",
    "                edge_map = []\n",
    "                for edge in roi.findall('.//ns:edgeMap', namespace):\n",
    "                    x_coord = int(edge.find('ns:xCoord', namespace).text)\n",
    "                    y_coord = int(edge.find('ns:yCoord', namespace).text)\n",
    "                    edge_map.append((x_coord, y_coord))\n",
    "                if image_sop_uid not in annotations_dict:\n",
    "                    annotations_dict[image_sop_uid] = []\n",
    "                annotation = {\n",
    "                    'nodule_id': nodule_id,\n",
    "                    'edge_map': edge_map\n",
    "                }\n",
    "                if characteristics_dict:\n",
    "                    annotation['characteristics'] = characteristics_dict\n",
    "                annotations_dict[image_sop_uid].append(annotation)\n",
    "    return annotations_dict\n",
    "\n",
    "all_annotations = {}\n",
    "\n",
    "for subdir, _, files in os.walk(xml_root_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".xml\"):\n",
    "            xml_file_path = os.path.join(subdir, file)\n",
    "            annotations = extract_annotations(xml_file_path)\n",
    "            for uid, annotation in annotations.items():\n",
    "                if uid not in all_annotations:\n",
    "                    all_annotations[uid] = []\n",
    "                all_annotations[uid].extend(annotation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sphericity Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "def process_dataset_sphericity(root_dir, output_file, all_annotations):\n",
    "    coco_dataset = {\n",
    "        \"images\": [],\n",
    "        \"annotations\": [],\n",
    "        \"categories\": [\n",
    "            {\"id\": 1, \"name\": \"Linear\"},\n",
    "            {\"id\": 2, \"name\": \"Ovoid/Linear\"},\n",
    "            {\"id\": 3, \"name\": \"Ovoid\"},\n",
    "            {\"id\": 4, \"name\": \"Ovoid/Round\"},\n",
    "            {\"id\": 5, \"name\": \"NA\"},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    annotation_id = 0\n",
    "    image_id = 0\n",
    "\n",
    "    for subdir, _, files in os.walk(root_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".jpg\"):\n",
    "                file_path = os.path.join(subdir, file)\n",
    "                \n",
    "                uid = os.path.splitext(file)[0]\n",
    "                \n",
    "                with Image.open(file_path) as img:\n",
    "                    width, height = img.size\n",
    "                    \n",
    "                    coco_dataset[\"images\"].append({\n",
    "                        \"id\": image_id,\n",
    "                        \"file_name\": file,\n",
    "                        \"width\": width,\n",
    "                        \"height\": height\n",
    "                    })\n",
    "                    \n",
    "                    if uid in all_annotations:\n",
    "                        for annotation in all_annotations[uid]:\n",
    "                            edge_points = annotation['edge_map']\n",
    "                            x, y = zip(*edge_points)\n",
    "                            bbox = [min(x), min(y), max(x) - min(x), max(y) - min(y)]\n",
    "                            area = bbox[2] * bbox[3]\n",
    "                            score = annotation['characteristics']['sphericity']\n",
    "                            coco_dataset[\"annotations\"].append({\n",
    "                                \"id\": annotation_id,\n",
    "                                \"image_id\": image_id,\n",
    "                                \"category_id\": score,\n",
    "                                \"bbox\": bbox,\n",
    "                                \"area\": area,\n",
    "                                \"segmentation\": [],\n",
    "                                \"iscrowd\": 0\n",
    "                            })\n",
    "                            annotation_id += 1\n",
    "                    \n",
    "                    image_id += 1\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(coco_dataset, f)\n",
    "\n",
    "train_dir = \"CT\\\\train\"\n",
    "val_dir = \"CT\\\\val\"\n",
    "test_dir = \"CT\\\\test\"\n",
    "\n",
    "train_output_file = os.path.join(train_dir, 'coco_dataset_train_sphericity.json')\n",
    "val_output_file = os.path.join(val_dir, 'coco_dataset_val_sphericity.json')\n",
    "test_output_file = os.path.join(test_dir, 'coco_dataset_test_sphericity.json')\n",
    "\n",
    "process_dataset_sphericity(train_dir, train_output_file, all_annotations)\n",
    "process_dataset_sphericity(val_dir, val_output_file, all_annotations)\n",
    "process_dataset_sphericity(test_dir, test_output_file, all_annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Margin Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "def process_dataset_margin(root_dir, output_file, all_annotations):\n",
    "    coco_dataset = {\n",
    "        \"images\": [],\n",
    "        \"annotations\": [],\n",
    "        \"categories\": [\n",
    "            {\"id\": 1, \"name\": \"Poorly Defined\"},\n",
    "            {\"id\": 2, \"name\": \"Near Poorly Defined\"},\n",
    "            {\"id\": 3, \"name\": \"Medium Margin\"},\n",
    "            {\"id\": 4, \"name\": \"Near Sharp\"},\n",
    "            {\"id\": 5, \"name\": \"Sharp\"},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    annotation_id = 0\n",
    "    image_id = 0\n",
    "\n",
    "    for subdir, _, files in os.walk(root_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".jpg\"):\n",
    "                file_path = os.path.join(subdir, file)\n",
    "                \n",
    "                uid = os.path.splitext(file)[0]\n",
    "                \n",
    "                with Image.open(file_path) as img:\n",
    "                    width, height = img.size\n",
    "                    \n",
    "                    coco_dataset[\"images\"].append({\n",
    "                        \"id\": image_id,\n",
    "                        \"file_name\": file,\n",
    "                        \"width\": width,\n",
    "                        \"height\": height\n",
    "                    })\n",
    "                    \n",
    "                    if uid in all_annotations:\n",
    "                        for annotation in all_annotations[uid]:\n",
    "                            edge_points = annotation['edge_map']\n",
    "                            x, y = zip(*edge_points)\n",
    "                            bbox = [min(x), min(y), max(x) - min(x), max(y) - min(y)]\n",
    "                            area = bbox[2] * bbox[3]\n",
    "                            score=annotation['characteristics']['margin']\n",
    "                            coco_dataset[\"annotations\"].append({\n",
    "                                \"id\": annotation_id,\n",
    "                                \"image_id\": image_id,\n",
    "                                \"category_id\": score,\n",
    "                                \"bbox\": bbox,\n",
    "                                \"area\": area,\n",
    "                                \"segmentation\": [],\n",
    "                                \"iscrowd\": 0\n",
    "                            })\n",
    "                            annotation_id += 1\n",
    "                    \n",
    "                    image_id += 1\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(coco_dataset, f)\n",
    "\n",
    "train_dir = \"CT\\\\train\"\n",
    "val_dir = \"CT\\\\val\"\n",
    "test_dir = \"CT\\\\test\"\n",
    "\n",
    "train_output_file = os.path.join(train_dir, 'coco_dataset_train_margin.json')\n",
    "val_output_file = os.path.join(val_dir, 'coco_dataset_val_margin.json')\n",
    "test_output_file = os.path.join(test_dir, 'coco_dataset_test_margin.json')\n",
    "\n",
    "process_dataset_margin(train_dir, train_output_file, all_annotations)\n",
    "process_dataset_margin(val_dir, val_output_file, all_annotations)\n",
    "process_dataset_margin(test_dir, test_output_file, all_annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Texture Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "def process_dataset_annotation(root_dir, output_file, all_annotations):\n",
    "    coco_dataset = {\n",
    "        \"images\": [],\n",
    "        \"annotations\": [],\n",
    "        \"categories\": [\n",
    "            {\"id\": 1, \"name\": \"Non-Solid/GGO\"},\n",
    "            {\"id\": 2, \"name\": \"Non-Solid/Mixed\"},\n",
    "            {\"id\": 3, \"name\": \"Part Solid/Mixed\"},\n",
    "            {\"id\": 4, \"name\": \"Solid/Mixed\"},\n",
    "            {\"id\": 5, \"name\": \"Solid\"},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    annotation_id = 0\n",
    "    image_id = 0\n",
    "\n",
    "    for subdir, _, files in os.walk(root_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".jpg\"):\n",
    "                file_path = os.path.join(subdir, file)\n",
    "                \n",
    "                uid = os.path.splitext(file)[0]\n",
    "                \n",
    "                with Image.open(file_path) as img:\n",
    "                    width, height = img.size\n",
    "                    \n",
    "                    coco_dataset[\"images\"].append({\n",
    "                        \"id\": image_id,\n",
    "                        \"file_name\": file,\n",
    "                        \"width\": width,\n",
    "                        \"height\": height\n",
    "                    })\n",
    "                    \n",
    "                    if uid in all_annotations:\n",
    "                        for annotation in all_annotations[uid]:\n",
    "                            edge_points = annotation['edge_map']\n",
    "                            x, y = zip(*edge_points)\n",
    "                            bbox = [min(x), min(y), max(x) - min(x), max(y) - min(y)]\n",
    "                            area = bbox[2] * bbox[3]\n",
    "                            score = annotation['characteristics']['texture']\n",
    "                            coco_dataset[\"annotations\"].append({\n",
    "                                \"id\": annotation_id,\n",
    "                                \"image_id\": image_id,\n",
    "                                \"category_id\": score,\n",
    "                                \"bbox\": bbox,\n",
    "                                \"area\": area,\n",
    "                                \"segmentation\": [],\n",
    "                                \"iscrowd\": 0\n",
    "                            })\n",
    "                            annotation_id += 1\n",
    "                    \n",
    "                    image_id += 1\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(coco_dataset, f)\n",
    "\n",
    "train_dir = \"CT\\\\train\"\n",
    "val_dir = \"CT\\\\val\"\n",
    "test_dir = \"CT\\\\test\"\n",
    "\n",
    "train_output_file = os.path.join(train_dir, 'coco_dataset_train_annotation.json')\n",
    "val_output_file = os.path.join(val_dir, 'coco_dataset_val_annotation.json')\n",
    "test_output_file = os.path.join(test_dir, 'coco_dataset_test_annotation.json')\n",
    "\n",
    "process_dataset_annotation(train_dir, train_output_file, all_annotations)\n",
    "process_dataset_annotation(val_dir, val_output_file, all_annotations)\n",
    "process_dataset_annotation(test_dir, test_output_file, all_annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Malignancy Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "def process_dataset_malignancy(root_dir, output_file, all_annotations):\n",
    "    coco_dataset = {\n",
    "        \"images\": [],\n",
    "        \"annotations\": [],\n",
    "        \"categories\": [\n",
    "            {\"id\": 1, \"name\": \"Highly Unlikely\"},\n",
    "            {\"id\": 2, \"name\": \"Moderately Unlikely\"},\n",
    "            {\"id\": 3, \"name\": \"Indeterminate\"},\n",
    "            {\"id\": 4, \"name\": \"Moderately Suspicious\"},\n",
    "            {\"id\": 5, \"name\": \"Highly Suspicious\"},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    annotation_id = 0\n",
    "    image_id = 0\n",
    "\n",
    "    for subdir, _, files in os.walk(root_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".jpg\"):\n",
    "                file_path = os.path.join(subdir, file)\n",
    "                \n",
    "                uid = os.path.splitext(file)[0]\n",
    "                \n",
    "                with Image.open(file_path) as img:\n",
    "                    width, height = img.size\n",
    "                    \n",
    "                    coco_dataset[\"images\"].append({\n",
    "                        \"id\": image_id,\n",
    "                        \"file_name\": file,\n",
    "                        \"width\": width,\n",
    "                        \"height\": height\n",
    "                    })\n",
    "                    \n",
    "                    if uid in all_annotations:\n",
    "                        for annotation in all_annotations[uid]:\n",
    "                            edge_points = annotation['edge_map']\n",
    "                            x, y = zip(*edge_points)\n",
    "                            bbox = [min(x), min(y), max(x) - min(x), max(y) - min(y)]\n",
    "                            area = bbox[2] * bbox[3]\n",
    "                            score = annotation['characteristics']['malignancy']\n",
    "                            coco_dataset[\"annotations\"].append({\n",
    "                                \"id\": annotation_id,\n",
    "                                \"image_id\": image_id,\n",
    "                                \"category_id\": score,\n",
    "                                \"bbox\": bbox,\n",
    "                                \"area\": area,\n",
    "                                \"segmentation\": [],\n",
    "                                \"iscrowd\": 0\n",
    "                            })\n",
    "                            annotation_id += 1\n",
    "                    \n",
    "                    image_id += 1\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(coco_dataset, f)\n",
    "\n",
    "train_dir = \"CT\\\\train\"\n",
    "val_dir = \"CT\\\\val\"\n",
    "test_dir = \"CT\\\\test\"\n",
    "\n",
    "train_output_file = os.path.join(train_dir, 'coco_dataset_train_malignancy.json')\n",
    "val_output_file = os.path.join(val_dir, 'coco_dataset_val_malignancy.json')\n",
    "test_output_file = os.path.join(test_dir, 'coco_dataset_test_malignancy.json')\n",
    "\n",
    "process_dataset_malignancy(train_dir, train_output_file, all_annotations)\n",
    "process_dataset_malignancy(val_dir, val_output_file, all_annotations)\n",
    "process_dataset_malignancy(test_dir, test_output_file, all_annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spiculation Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "def process_dataset_spiculation(root_dir, output_file, all_annotations):\n",
    "    coco_dataset = {\n",
    "        \"images\": [],\n",
    "        \"annotations\": [],\n",
    "        \"categories\": [\n",
    "            {\"id\": 1, \"name\": \"No Spiculation\"},\n",
    "            {\"id\": 2, \"name\": \"Nearly No Spiculation\"},\n",
    "            {\"id\": 3, \"name\": \"Medium Spiculation\"},\n",
    "            {\"id\": 4, \"name\": \"Near Marked Spiculation\"},\n",
    "            {\"id\": 5, \"name\": \"Marked Spiculation\"},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    annotation_id = 0\n",
    "    image_id = 0\n",
    "\n",
    "    for subdir, _, files in os.walk(root_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".jpg\"):\n",
    "                file_path = os.path.join(subdir, file)\n",
    "                \n",
    "                uid = os.path.splitext(file)[0]\n",
    "                \n",
    "                with Image.open(file_path) as img:\n",
    "                    width, height = img.size\n",
    "                    \n",
    "                    coco_dataset[\"images\"].append({\n",
    "                        \"id\": image_id,\n",
    "                        \"file_name\": file,\n",
    "                        \"width\": width,\n",
    "                        \"height\": height\n",
    "                    })\n",
    "                    \n",
    "                    if uid in all_annotations:\n",
    "                        for annotation in all_annotations[uid]:\n",
    "                            edge_points = annotation['edge_map']\n",
    "                            x, y = zip(*edge_points)\n",
    "                            bbox = [min(x), min(y), max(x) - min(x), max(y) - min(y)]\n",
    "                            area = bbox[2] * bbox[3]\n",
    "                            score = annotation['characteristics']['spiculation']\n",
    "                            coco_dataset[\"annotations\"].append({\n",
    "                                \"id\": annotation_id,\n",
    "                                \"image_id\": image_id,\n",
    "                                \"category_id\": score,\n",
    "                                \"bbox\": bbox,\n",
    "                                \"area\": area,\n",
    "                                \"segmentation\": [],\n",
    "                                \"iscrowd\": 0\n",
    "                            })\n",
    "                            annotation_id += 1\n",
    "                    \n",
    "                    image_id += 1\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(coco_dataset, f)\n",
    "\n",
    "train_dir = \"CT\\\\train\"\n",
    "val_dir = \"CT\\\\val\"\n",
    "test_dir = \"CT\\\\test\"\n",
    "\n",
    "train_output_file = os.path.join(train_dir, 'coco_dataset_train_spiculation.json')\n",
    "val_output_file = os.path.join(val_dir, 'coco_dataset_val_spiculation.json')\n",
    "test_output_file = os.path.join(test_dir, 'coco_dataset_test_spiculation.json')\n",
    "\n",
    "process_dataset_spiculation(train_dir, train_output_file, all_annotations)\n",
    "process_dataset_spiculation(val_dir, val_output_file, all_annotations)\n",
    "process_dataset_spiculation(test_dir, test_output_file, all_annotations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
